<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Prediction Html</title>
</head>
<body>
<h1>
    Predictive maintenance
</h1>
<p>
    Predictive maintenance techniques are designed to help determine the condition of in-service equipment in order to
    predict when maintenance should be performed. This approach promises cost savings over routine or time-based
    preventive maintenance, because tasks are performed only when warranted.
</p>
<p>
    Our predictive maintenance template focuses on predicting when an in-service machine will fail,
    so this would help maitenance to be planned in advance. This project includes python scripts using libraries such as
    <b>pandas</b>, <b>numpy</b> and <b>sklearn</b>.
</p>
<p>
    In this project we provided three modeling solutions to accomplish tasks as:<br>
    &#9656; <b>Regression</b> - to predict the Remaining Useful Life (RUL), or Time to Failure(TTF).<br>
    &#9656; <b>Binary classification</b> - to predict if an asset will fail within certain time frame(e.g. days).<br>
    &#9656; <b>Multi-class classification</b> - to predict if an asset will fail in different time windows: E.g., fails in
    window[1,<i>w0</i>] days; fails in the window [<i>w0</i>+1,<i>w1</i>] days; not fail within <i>w1</i> days
</p>
<p>
    The project uses the example of simulated aircraft engine run-to-failure events to demonstrate the predictive
    maintenance modeling process. The degradation pattern is reflected by the asset's sensor measurements. By examining the
    asset's sensor values over time, the machine learning algorithm can learn the relationship between the sensor
    values and changes in sensor values to the historical failures in order to predict failures in the future.
</p>
<p>
    You can download the test data from and try our project following the steps below.<br>
    Download <a href="../PM_test_sample.txt" download>PM_test_sample.txt</a> or <a href="../PM_test.txt" download>PM_test_.txt</a>
</p>
<h2>How to test the project</h2>
 <p>
     First of all, the datasets used are from NASA aircraft engine and it contains data coming from 21 sensors.
 </p>
<p>
    Step 1: Make sure you downloaded the test data that we provide.<br>
    We have two samples one that contains the RUL column <a href="../PM_test_sample.txt" download>PM_test_sample.txt</a> and another one without that column <a href="../PM_test.txt" download>PM_test.txt</a>.<br>
    The difference between those files is that the previous one has the column RUL which is the actual value that has to be predicted by the algorithms, so you could verify the predicted values with the actual values from RUL.

</p>
<p>
    Step 2: Upload the test data file you previously downloaded.
</p>
<form action="http://predictivemaintenance.azurewebsites.net/predict" method="post" enctype="multipart/form-data">
    <input type="file" name="file" id="file" accept=".csv,.txt">
    <input type="submit" value="Upload csv" name="submit">
</form>
<p>
    Step 3: Press the 'Upload csv' button so the file can be processed and then a file containing the outcome will be downloaded on your local machine so you can visualize the predicted data.
    At first sight maybe you won't understand much, but that's not a problem, below you'll find the significance of each column.
</p>
<p>
    Columns for the export file:<br>
    &#9642; id: since we have 100 engines , values will be between 1 - 100<br>
    &#9642; cycle: is our time unit for this project<br>
    &#9642; s1 - s21: sensor readings for each cycle<br>
    &#9642; binary_classification_*: represent the binary classification made by different algorithms like Decision Tree,
    Logistic Regression, Neural Network, Random Forest<br>
    &#9642; binary_classification_Sampled_AdaBoostDecisionTreeClassifier: this one represents the outcome of the algorithm
    Decision Tree but it was trained using half of the data as failure engine and half as working engine<br>
    &#9642; multi-class: represent multi-class classification made by different algorithms like Decision Tree, Logistic Regression,
    Neural Network, Random Forest<br>
    &#9642; multi-class_lr_ordinal_regression: represent ordinal regression for Logistic Regression<br>
    &#9642; multi-class_nn_ordinal_regression: represent ordinal regression for Neural Network<br>
    &#9642; regression_*: represent the predicted values made by following algorithms Decision Forest, Gradient Boosting, Neural Network, Poisson Regression<br>
    &#9642; RUL: this is the actual value and represents the "Remaining Useful Life". This value will be predicted by the last cycle for each id (engine) e.g. id = 1 will predict for RUL = 112
</p>
<p>
    The only thing remains is to try with your own test dataset, and for that we would like you to contact us at this email address <i>qubiz@support.com</i> .
</p>
<p>
    Next if you are interested more you can read the how we prepare the date, train the algorithms and evaluate the data. This is divided into two parts.
</p>
<h2>1. Data preparation and feature engineering</h2>
<p>
    The template takes three datasets as inputs.<br>
    &#9656; Training data: It is the aircraft engine run-to-failure data.<br>
    &#9656; Testing data: It is the aircraft engine operating data without failure events recorded.<br>
    &#9656; Truth data: It contains the information of true remaining cycles for each engine in the testing data.
</p>
<p>
    The training data consists of multiple multivariate times series with "cycles" as the time unit, together with 21 sensor reading for each cycle.
    In this simulated data, the engine is assumed to be operating normally at the start of each time series.
    It starts to degrade at some point during the series of the operating cycles.
    When a predefined threshold is reached, then the engine is considered unsafe for further operation.
    <b>In other words, the last cycle in each time series can be considered as the failure point of the corresponding engine.</b>
    Taking the sample training data shown in the following table as an example, the engine with id = 1 fails at cycle 192, and the engine with id = 2 fails at cycle 287.
</p>
<img src="../img/train_data.jpg"><br><br>
<img src="../img/test_data.jpg"><br><br>
<img src="../img/RUL.jpg"><br><br>
<p>
    The testing data is similar to the training data, having the same data scheme. We find one difference train and test data which is that the test data does not indicate when the failure occurs,
    which means that this data doesn't show how many more cycles an engine can last before it fails.
</p>
<p>
    The truth data provides the number of remaining working cycles for the engines in the testing data, or as we call it remaining useful life.
</p>
<h4>Data Labeling</h4>
<p>
    Given the input data description from above there is one question we need to ask: "Having the operation and failure events history of the aircraft engine,
    can we predict when an in-service engine will fail?"
</p>
<p>This question can be re-formulate into three closely relevant questions, but to answer on each we'll be using three different types of machine learning models.</p>
<p>
    &#9656; Regression models: How many more cycles an in-service engine will last before it fails?<br>
    &#9656; Binary classification: Is this engine going to fail within <i>w1</i> cycles?<br>
    &#9656; Multi-class classification: Is this engine going to fail within the window [1, <i>w0</i>] cycles or to fail within
    the window [<i>w0</i>+1, <i>w1</i>] cycles, or it will not fail within <i>w1</i> cycles?
</p>
<p>
    We labeled the training data with "RUL", "label1" and "label2" which are for regression, binary classification,
    and multi-class classification models. In the following figure we show an example for engine with id=1. Here <i>w0</i> and <i>w1</i>
    are predefined use case related parameters which are used to label the training data. The customer needs to decide
    how far ahead of time the alert of failure should trigger before the actual failure event.
</p>
<h4>Feature engineering</h4>
<p>
    Another important task is to generate training and testing features.
</p>
<img src="../img/feature_engineering.jpg" height="700" width="601">
<p>
    The features are included or created in the training data can be grouped into two categories.<br>
    &#9656; Selected raw features<br>
    &#9656; Aggregate features
</p>
<p>
    <b>Selected raw features</b> - the raw features are those that are included in the original input data. All the sensor measurements (s1-s21) are included in the training data and
    other raw features used are: cycle, setting1-setting3.
</p>
<p>
    <b>Aggregate features</b> - these features summarize the historical activity of each asset. We created
    two types of aggregate features for each of the 21 sensors. The description is as follows:<br>
    &#9656; a1-a21: the moving average of sensor values in the most <i>w</i> recent cycles<br>
    &#9656; sd1-sd21: the standard deviation of sensor values in the most <i>w</i> recent cycles
</p>
<h4>Prepare the testing data</h4>
<p>
    To aggregate features during the feature engineering process we will be using the time series data in the testing data.
    In the testing data, we only keep the the record with the maximum cycle for each engine id.

    In other words, one record is retained for each engine. In the end, the testing data contains 100 records, which matches the RUL in the truth data.
</p>
<h2>2. Train and evaluate model</h2>
<p>
    The output datasets of part 1 will serve as the input datasets of part 2.<br>
    This part consists of three parallel steps, each of which has a separate python script. The names of these experiments are shown below.<br>
    &#9656; Train and evaluate regression models<br>
    &#9656; Train and evaluate binary classification models<br>
    &#9656; Train and evaluate multi-class classification models<br>
</p>
<p>
    There are several common steps when training three different types of models. We illustrate these similar steps in detail as follows.
</p>
<p>
    In the first place, they share the same training data and testing data. Secondly, we will exclude irrelevant label columns for each model.
    For example, the first step will exclude columns "label1" and "label2" and keep only the label "RUL" in the training data to prepare train regression models on "RUL" column.
    Finally, we select top 35 correlated features in the training data based on "Pearson Correlation" measure.
</p>
<h4>Train and evaluate regression models</h4>
<p>
    In this step, we train and evaluate four regression models: Decision Forest Regression, Boosted Decision Tree Regression, Poisson Regression, and Neural Network Regression.
</p>
<img src="../img/regression_output_prediction.jpg">
<h4>Train and evaluate binary classification models</h4>
<p>
    In this step, we train and evaluate four binary classification models: Two-Class Logistic Regression, Two-Class Boosted Decision Tree, Two-Class Decision Forest, and Two-Class Neural Network.
</p>
<p>
    The following figure compares the results from multiple models to determine the best model. The algorithm "Two-Class Neural Network" performs best in terms of four metrics: "Accuracy",
    "Precision", "Recall", and "F-Score".
</p>
<img src="../img/bin_classif_prediction.jpg">
<h4>Train and evaluate multi-class classification models</h4>
<p>
    In this step, we train and evaluate two multi-class classification models: Multi-class Logistic Regression and Multi-class Neural Network,
    and two ordinal regression models on Two-Class Logistic Regression and Two-Class Neural Network. Ordinal regression is a type of regression analysis used to predict an ordinal variable.
    An ordinal variable is the variable whose value can be ranked or ordered, but the real distance between these values is unknown.
    In the multi-class classification problem formulated here, the class attribute "label2" is an ordinal variable, as its value reflects the severity of the failure progress.
</p>
<p>
    The following figures compares the results of "Multi-class Logistic Regression" and "Multi-class Neural Network", where the latter performs better in term of six metrics:
    "Overall accuracy", "Average accuracy",  "Micro-averaged precision", "Macro-averaged precision", "Micro-averaged recall", and "Macro-averaged recall".
</p>
<img src="../img/multiclass_prediction.jpg">
<p>
</p>
</body>
<footer>
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
</footer>
</html>